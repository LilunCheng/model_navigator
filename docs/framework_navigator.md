<!--
Copyright (c) 2021, NVIDIA CORPORATION. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# Framework Navigator
Framework Navigator provides python API that supports user with exporting model from source code.
The main goal of the project is to automate model deployment and validation. Process starts with single API call
that run model [export](#Model-export), [conversion](#Model-conversion), [correctness test](#Correctness-test) and [performance evaluation](#Performance-evaluation).
We are encouraging users to use Framework Navigator before and after model training to test architecture
and how it behaves after export and conversion. Integration of Framework Navigator in the earliest phases of model
development helps with catching errors and possible issues before deploying model in production environment.

* [Requirements](#Requirements)
* [Installation](#Installation)
* [Export process](#requirements)
  * [Model export](#Model-export)
  * [Model conversion](#Model-conversion)
  * [Correctness test](#Correctness-test)
  * [Performance evaluation](#Performance-evaluation)
* [Results](#Results)
  * [Navigator package](#Navigator-package)
  * [Package descriptor](#Package-descriptor)
* [Export process](#Export-process)
  * [PyTorch export](#PyTorch-export)
  * [PyTorch package descriptor](#PyTorch-package-descriptor)
  * [PyTorch model verification](#PyTorch-model-verification)
  * [TensorFlow 2 export](#TensorFlow-2-export)
* [Troubleshooting](#Troubleshooting)
* [Export API](#export-api)
  * [PyTorch](#PyTorch)
  * [TensorFlow 2](#TensorFlow-2)
* [Package descriptor API](#Package-descriptor-API)


## Requirements
 - Environment with TensorFlow 2 for TF2 models export
 - Environment with PyTorch for PyTorch models export

## Installation

Framework Navigator is installed with Model Navigator package.
To install Framework Navigator without Model Navigator dependencies use commands:

```shell
$ git clone <model_navigator_repo>
$ cd model_navigator
```

Framework Navigator for PyTorch:
```shell
$ pip install .[pyt]
```
Framework Navigator for TensorFlow 2:
```shell
$ pip install .[tf]
```

## Exporting models
Framework Navigator export process is based on commands that are executed in order.
Steps described below represents core functionality related to export and model verification.


### Model export
Exports model from framework source code to binary format.

Supported output formats for TensorFlow2:
- SavedModel

Supported output formats for PyTorch:
- ONNX
- Torch-TensorRT
- TorchScript

### Model conversion
Convert model from export format to target format.
We perform the set of conversion to test functionality. For optimal performance you have to do conversion on
production target with Model Navigator CLI.

Supported conversions for TensorFlow2:
- SavedModel to TensorFlow-TensorRT SavedModel

Supported conversions for PyTorch:
- ONNX to TensorRT

### Correctness test
Comparison between outputs from exported model and source model.
Implementation is based on numpy.allclose:
https://numpy.org/doc/stable/reference/generated/numpy.allclose.html

If absolute tolerance (atol) and relative tolerance (rtol) are not provided by the user, tolerance values will be calculated
based on models outputs.

- Absolute tolerance is calculated as element-wise maximal difference between values in source and exported model outputs.
- Relative tolerance is calculated as Mean Squared Error between source and exported model outputs.

### Performance evaluation
After conversions Framework Navigator performs a set of performance tests in different configurations to
verify if given format has proper performance optimizations and provides speedup.

## Results
All information about export, conversion and optimizations are encapsulated into cohered package
that can be used later for deployment in production, feature request, bug reports, reproduction.

### Navigator package
Export results are stored inside ```navigator_workdir``` directory in ```<model_name>.nav``` package.

```model_input``` directory contains input samples generated by dataloader saved to Numpy npz format.

```model_output``` directory contains output data returned from model saved to Numpy npz format.

```navigator.log``` contains logs generated by Framework Navigator and error message from run.

```status.yaml``` contains status of exported models, Framework Navigator configuration and information about environment.

Each format related directory contains exported model and ```config.yaml``` that can be used as input for Model Navigator CLI.

```
navigator_workdir/
└── navigator_model.nav
    ├── status.yaml
    ├── navigator.log
    ├── model_input
    │   ├── sample_0.npz
    │   ├── sample_1.npz
    │   ├── sample_2.npz
    │   ├── sample_3.npz
    │   ├── sample_4.npz
    ├── model_output
    │   ├── sample_0.npz
    │   ├── sample_1.npz
    │   ├── sample_2.npz
    │   ├── sample_3.npz
    │   ├── sample_4.npz
    ├── onnx
    │   ├── config.yaml
    │   └── model.onnx
    ├── torch-trt
    │   ├── config.yaml
    │   └── model.pt
    ├── torchscript-script
    │   ├── config.yaml
    │   └── model.pt
    └── torchscript-trace
        ├── config.yaml
        └── model.pt
```

### Package descriptor
If after export you still want to work on your model, framework navigator gives you nice interface to check the status
and interact with different parts of exported and converted formats.
Additionally, model verification tests (e.g. accuracy, business metrics) cannot be done automatically. User should
verify converted models after exporting them, by setting the verified state for particular formats after executing custom
test scenarios.

## Export process
Below are export API usage examples for PyTorch and TensorFlow 2. This is the correct way to invoke Framework Navigator
and start export process.

### PyTorch export
Source code shown below exports model to ONNX and TorchScript format.

```python
import torch
import model_navigator.framework_api as nav

device = "cuda" if torch.cuda.is_available() else "cpu"

def dataloader():
    for _ in range(10):
        yield torch.full((3, 5), 9, device=device)

model = torch.nn.Linear(5, 7).to(device).eval()

nav.torch.export(
    model=model,
    model_name="example_model",
    dataloader=dataloader,
    opset=13,
    input_names=("input",),
    dynamic_axes={"input": {0: "batch"}},
)
```

### PyTorch package descriptor
Export model to  ONNX format and check if export status is successful.
If model exported correctly, load it and return model handle.
```python
import torch
import model_navigator.framework_api as nav

device = "cuda" if torch.cuda.is_available() else "cpu"

def dataloader():
    for _ in range(10):
        yield torch.full((3, 5), 9, device=device)

model = torch.nn.Linear(5, 7).to(device).eval()

package_desc = nav.torch.export(
    model=model,
    model_name="example_model",
    dataloader=dataloader,
    target_formats=(nav.Format.ONNX,),
    opset=13,
    input_names=("input",),
    dynamic_axes={"input": {0: "batch"}},
)

onnx_status = package_desc.get_status(format=nav.Format.ONNX)
if onnx_status:
    onnx_model = package_desc.get_model(format=nav.Format.ONNX)
```

### PyTorch model verification
Export model to TorchScript format and then verify model accuracy.
If accuracy verification passed set model format as verified.
```python
import torch
import model_navigator.framework_api as nav

device = "cuda" if torch.cuda.is_available() else "cpu"

def dataloader():
    for _ in range(10):
        yield torch.full((3, 5), 9, device=device)

ground_truth = next(dataloader())

model = torch.nn.Identity().to(device).eval()

package_desc = nav.torch.export(
    model=model,
    model_name="example_model",
    dataloader=dataloader,
    target_formats=(nav.Format.TORCHSCRIPT,),
    jit_options=(nav.JitType.SCRIPT,),
    opset=13,
    input_names=("input",),
    dynamic_axes={"input": {0: "batch"}},
)

ts_status = package_desc.get_status(format=nav.Format.TORCHSCRIPT, jit_type=nav.JitType.SCRIPT)
if ts_status:
    ts_model = package_desc.get_model(format=nav.Format.TORCHSCRIPT, jit_type=nav.JitType.SCRIPT)
    inputs = next(dataloader())
    outputs = ts_model(inputs)
    is_model_valid = torch.allclose(inputs, outputs, atol=0.0, rtol=0.0)

    if is_model_valid:
        package_desc.set_verified(format=nav.Format.TORCHSCRIPT, jit_type=nav.JitType.SCRIPT)
```


### TensorFlow 2 export
Source code shown below exports model to Savedmodel and TF-TRT formats.

```python
import tensorflow as tf
import model_navigator.framework_api as nav

def dataloader():
    yield tf.random.uniform(shape=[1, 224, 224, 3], minval=0, maxval=1, dtype=tf.dtypes.float32),

inp = tf.keras.layers.Input((1, 224, 224, 3))
layer_output = tf.keras.layers.Lambda(lambda x: x)(inp)
layer_output = tf.keras.layers.Lambda(lambda x: x)(layer_output)
layer_output = tf.keras.layers.Lambda(lambda x: x)(layer_output)
layer_output = tf.keras.layers.Lambda(lambda x: x)(layer_output)
layer_output = tf.keras.layers.Lambda(lambda x: x)(layer_output)
model_output = tf.keras.layers.Lambda(lambda x: x)(layer_output)
model = tf.keras.Model(inp, model_output)

package_desc = nav.tensorflow.export(
    model=model,
    dataloader=dataloader,
    override_workdir=True,
    keep_workdir=True,
)
```

## Troubleshooting
Some operations may fail during Framework Navigator execution. In that case user will be informed about failing operations.
Additional information (e.g. exception messages) can be find in ```naviagotor.log``` located inside
```<model_name>.nav``` package.

Below you can see example log for failing ONNX correctness test. At the end of the log you can find brief summary of
Framework Navigator execution.
At the beginning you can find history of all steps executed in the run. Correctness test failed because of AssertionError,
there is also information about current tolerance (atol, rtol) and suggestion to adjust those values.

```shell
2022-02-22 15:34:28 INFO     Navigator API: ============================== Pipeline PyTorch pipeline started ===================================
2022-02-22 15:34:28 INFO     Navigator API: ============================== Command Fetch input model data started ==============================
2022-02-22 15:34:28 INFO     Navigator API: ============================== Command Export PyTorch to ONNX started ==============================
2022-02-22 15:34:32 INFO     Navigator API: ============================== Command Correctness PyTorch to ONNX started =========================
2022-02-22 15:34:33 ERROR    Navigator API: Traceback (most recent call last):
  File "/model_navigator/model_navigator/framework_api/commands/core.py", line 85, in transform
    results = self.__call__(**kwargs)
  File "/model_navigator/model_navigator/framework_api/commands/correctness/base.py", line 63, in __call__
    assert all(all_close_checks), get_assert_message(atol, rtol)
AssertionError: Current atol = 0.001, rtol = 0.001, try to adjust tolerance values

2022-02-22 15:34:33 INFO     Navigator API: You can disable error suppression for debugging with flag NAV_DEBUG=1
2022-02-22 15:34:33 INFO     Navigator API: ============================== Command Performance ONNX started ====================================
2022-02-22 15:34:33 INFO     Navigator API: ============================== Command Generate configurations for Navigator CLI started ===========
2022-02-22 15:34:33 INFO     Navigator API: ============================== Command Dump input model data started ===============================
2022-02-22 15:34:33 INFO     Navigator API: ============================== Command Dump output model data started ==============================
2022-02-22 15:34:33 INFO     Navigator API: ============================== Framework Navigator summary =========================================
2022-02-22 15:34:33 INFO     Navigator API: ============================== Pipeline PyTorch pipeline summary ===================================
2022-02-22 15:34:33 INFO     Navigator API: [ OK ] Fetch input model data
2022-02-22 15:34:33 INFO     Navigator API: [ OK ] Export PyTorch to ONNX
2022-02-22 15:34:33 WARNING  Navigator API: [FAIL] Correctness PyTorch to ONNX
2022-02-22 15:34:33 INFO     Navigator API: [ OK ] Performance ONNX
2022-02-22 15:34:33 INFO     Navigator API: [ OK ] Generate configurations for Navigator CLI
2022-02-22 15:34:33 INFO     Navigator API: [ OK ] Dump input model data
2022-02-22 15:34:33 INFO     Navigator API: [ OK ] Dump output model data
2022-02-22 15:34:35 WARNING  Navigator API: Initially models are not verified. Validate exported models and use PackageDescriptor.set_verified(format, jit_type, precision) method to set models as verified.
```



## Export API
### PyTorch

```python
def export(
    model, # model instance
    dataloader: Callable, # function returning generator
    model_name: Optional[str] = None,
    opset: Optional[int] = None, # ONNX opset, by default latest is used
    target_formats: Optional[Tuple[Format]] = None,
    target_precisions: Optional[Tuple[Precision]] = None,
    max_workspace_size: Optional[int] = None,
    jit_options: Optional[Tuple[JitType]] = None,
    workdir: Optional[Path] = None, # default workdir is navigator_workdir in current working directory
    override_workdir: bool = False,
    keep_workdir: bool = True,
    sample_count: Optional[int] = None, # number of samples that will be saved from dataloader
    atol: Optional[float] = None, # absolute tolerance used for correctness tests. If None, value will be calculated during run
    rtol: Optional[float] = None, # relative tolerance used for correctness tests. If None, value will be calculated during run
    input_names: Optional[Tuple[str]] = None, # model input name in the same order as in samples returned from dataloader
    dynamic_axes: Optional[Dict[str, Union[Dict[int, str], List[int]]]] = None, # for ONNX export, see https://pytorch.org/docs/1.9.1/onnx.html#functions
    target_device: Optional[str] = None, # target device for exporting the model
) -> PackageDescriptor:
    """Function exports PyTorch model to all supported formats."""
```

### TensorFlow 2
```python
def export(
    model,
    dataloader: Callable, # function returning generator
    target_precisions: Optional[Tuple[Precision]] = None,
    max_workspace_size: Optional[int] = None,
    minimum_segment_size: int = 3,
    model_name: Optional[str] = None,
    target_formats: Optional[Tuple[Format]] = None,
    workdir: Optional[Path] = None, # default workdir is navigator_workdir in current working directory
    override_workdir: bool = False,
    keep_workdir: bool = True,
    sample_count: Optional[int] = None, # number of samples that will be saved from dataloader
    atol: float = 0, # absolute tolerance used for correctness tests. If None, value will be calculated during run
    rtol: float = 0, # relative tolerance used for correctness tests. If None, value will be calculated during run
) -> PackageDescriptor:
    """Exports TensorFlow 2 model to all supported formats."""
```

## Package Descriptor API
```python
def get_formats_status(self) -> Dict:
    """Return dictionary of pairs Format : Bool. True for successful exports, False for failed exports."""
```


```python
def get_status(self, format: Format, jit_type: Optional[JitType] = None, precision: Optional[Precision] = None) -> bool:
    """Return status (True or False) of export operation for particular format, jit_type and precision."""
```


```python
def get_model(self, format: Format, jit_type: Optional[JitType] = None, precision: Optional[Precision] = None):
    """Load exported model for given format, jit_type and precision and return model object"""
```
```python
def set_verified(self, format: Format, jit_type: Optional[JitType] = None, precision: Optional[TensorRTPrecision] = None):
    """Set exported model verified for given format, jit_type and precision"""
```

