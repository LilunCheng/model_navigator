<!--
Copyright (c) 2021, NVIDIA CORPORATION. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# Framework Navigator
Framework Navigator provides python API that supports user with exporting model from source code to binary format.
After execution, it provides summary with information about successful exports and missing parameters.

After run package containing models and additional meta data will be generated.

## Requirements
 - Environment with TensorFlow 2 for TF2 models export
 - Environment with PyTorch for PyTorch models export

# Quick start

## Installation

```cd model_navigator```

```pip install .``` will install Model Navigator with minimal set of dependencies.

```pip install .[cli]``` will install Model Navigator with set of dependencies that are required also by
Model Navigator CLI.

### Navigator package
Results are stored inside navigator workdir in <model_name>.nav package.

```model_input``` directory contains input samples generated by dataloader saved to Numpy npz format.

```model_output``` directory contains output data returned from model saved to Numpy npz format.

```navigator.log``` contains logs generated by Framework Navigator and error message from run.

```status.yaml``` contains status of exported models, Framework Navigator configuration and information about environment.

Each format related directory contains exported model and ```config.yaml``` that can be used as input for Model Navigator CLI.

```
navigator_workdir/
└── navigator_model.nav
    ├── model_input
    │   ├── sample_0.npz
    │   ├── sample_1.npz
    │   ├── sample_2.npz
    │   ├── sample_3.npz
    │   ├── sample_4.npz
    ├── model_output
    │   ├── sample_0.npz
    │   ├── sample_1.npz
    │   ├── sample_2.npz
    │   ├── sample_3.npz
    │   ├── sample_4.npz
    ├── navigator.log
    ├── onnx
    │   ├── config.yaml
    │   └── model.onnx
    ├── status.yaml
    ├── torch-trt
    │   ├── config.yaml
    │   └── model.pt
    ├── torchscript-script
    │   ├── config.yaml
    │   └── model.pt
    └── torchscript-trace
        ├── config.yaml
        └── model.pt
```

### Examples
#### PyTorch
Source code shown below exports model to ONNX binary format and Torch binary format with Tracing and Scripting.

```python
import torch.nn as nn
import model_navigator.framework_api as nav

def dataloader():
    yield torch.randn(1)

class MyModule(nn.Module):
    def forward(self, x):
        return x + 10

model = MyModule()

nav.torch.export(
    model=model,
    dataloader=dataloader,
    override_workdir=True,
)
```

#### TensorFlow 2
Source code shown below exports model to Savedmodel binary format and TF-TRT Savedmodel binary format.
Then exported SavedModel is checked against ground truth and set as verified.

```python
import tensorflow as tf
import model_navigator.framework_api as nav

def dataloader():
    yield tf.random.uniform(shape=[1, 224, 224, 3], minval=0, maxval=1, dtype=tf.dtypes.float32),

inp = tf.keras.layers.Input((1, 224, 224, 3))
layer_output = tf.keras.layers.Lambda(lambda x: x)(inp)
layer_output = tf.keras.layers.Lambda(lambda x: x)(layer_output)
layer_output = tf.keras.layers.Lambda(lambda x: x)(layer_output)
layer_output = tf.keras.layers.Lambda(lambda x: x)(layer_output)
layer_output = tf.keras.layers.Lambda(lambda x: x)(layer_output)
model_output = tf.keras.layers.Lambda(lambda x: x)(layer_output)
model = tf.keras.Model(inp, model_output)

package_desc = nav.tensorflow.export(
    model=model,
    dataloader=dataloader,
    override_workdir=True,
)

exported_sm = package_desc.get_model(format=nav.Format.TF_SAVEDMODEL)

# Calculate metrics for your model (e.g. accuracy) and set model verified if results are correct.

x = next(dataloader())
y = exported_sm.predict(x)
ground_truth = "fake_ground_truth"

if y == ground_truth
    package_desc.set_verified(format=nav.Format.TF_SAVEDMODEL)
```

# API details

#### PyTorch export function

```python
def export(
    model, # model instance
    dataloader: Callable, # function returning generator
    model_name: Optional[str] = None,
    opset: Optional[int] = None, # ONNX opset, by default latest is used
    target_formats: Optional[Tuple[Format]] = None,
    jit_options: Optional[Tuple[JitType]] = None,
    workdir: Optional[Path] = None, # default workdir is navigator_workdir in current working directory
    override_workdir: bool = False,
    keep_workdir: bool = True,
    sample_count: Optional[int] = None, # number of samples that will be saved from dataloader
    atol: Optional[float] = None, # absolute tolerance used for correctness tests. If None, value will be calculated during run
    rtol: Optional[float] = None, # relative tolerance used for correctness tests. If None, value will be calculated during run
    input_names: Optional[Tuple[str]] = None, # model input name in the same order as in samples returned from dataloader
    dynamic_axes: Optional[Dict[str, Union[Dict[int, str], List[int]]]] = None, # for ONNX export, see https://pytorch.org/docs/1.9.1/onnx.html#functions
) -> PackageDescriptor:
    """Function exports PyTorch model to all supported formats."""
```

#### TensorFlow 2 export function
```python
def export(
    model,
    dataloader: Callable, # function returning generator
    target_precisions: Optional[Tuple[Precision]] = None,
    max_workspace_size: Optional[int] = None,
    minimum_segment_size: int = 3,
    model_name: Optional[str] = None,
    target_formats: Optional[Tuple[Format]] = None,
    workdir: Optional[Path] = None, # default workdir is navigator_workdir in current working directory
    override_workdir: bool = False,
    keep_workdir: bool = True,
    sample_count: Optional[int] = None, # number of samples that will be saved from dataloader
    atol: float = 0, # absolute tolerance used for correctness tests. If None, value will be calculated during run
    rtol: float = 0, # relative tolerance used for correctness tests. If None, value will be calculated during run
) -> PackageDescriptor:
    """Exports TensorFlow 2 model to all supported formats."""
```


#### PackageDescriptor:
User can query status of exported models or load model objects with help of the PackageDescriptor
```python
def get_formats_status(self) -> Dict:
    """Return dictionary of pairs Format : Bool. True for successful exports, False for failed exports."""
```


```python
def get_status(self, format: Format, jit_type: Optional[JitType] = None, precision: Optional[Precision] = None) -> bool:
    """Return status (True or False) of export operation for particular format, jit_type and precision."""
```


```python
def get_model(self, format: Format, jit_type: Optional[JitType] = None, precision: Optional[Precision] = None):
    """Load exported model for given format, jit_type and precision and return model object"""
```
```python
def set_verified(self, format: Format, jit_type: Optional[JitType] = None, precision: Optional[TensorRTPrecision] = None):
    """Set exported model verified for given format, jit_type and precision"""
```





